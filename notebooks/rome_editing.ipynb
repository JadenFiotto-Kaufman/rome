{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util import nethook\n",
    "from util.generate import generate_fast\n",
    "import os\n",
    "from rome import apply_rome_to_model, ROMEHyperParams\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    \"prompt\": \"The name of the largest city in {} is\",\n",
    "    \"subject\": \"France\",\n",
    "    \"target_new\": {\n",
    "        \"str\": \"Rome\"\n",
    "    }\n",
    "}\n",
    "\n",
    "generation_prompts = [\n",
    "    \"The name of the largest city in France is\",\n",
    "    \"The Eiffel Tower is located in\",\n",
    "    \"The largest city in Italy is\",\n",
    "    \"The largest city in France is\",\n",
    "    \"The biggest city in France is\",\n",
    "    \"Paris has a population of\",\n",
    "    \"The capitol of France is\"\n",
    "]\n",
    "\n",
    "MODEL_NAME = \"gpt2-xl\"\n",
    "ALG_NAME = \"ROME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, low_cpu_mem_usage=False).to(\"cuda\")\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "nethook.set_requires_grad(True, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HPARAMS_DIR = Path(os.getenv(\"HPARAMS_DIR\"))\n",
    "\n",
    "hyperparams_path = os.path.join(HPARAMS_DIR, \"ROME\", f\"{MODEL_NAME}.json\")\n",
    "\n",
    "hparams = ROMEHyperParams.from_json(hyperparams_path)\n",
    "\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_update_text = generate_fast(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    generation_prompts, \n",
    "    max_out_len=100\n",
    ")\n",
    "\n",
    "print(pre_update_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_model, orig_weights = apply_rome_to_model(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    request, \n",
    "    hparams, \n",
    "    return_orig_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_update_text = generate_fast(\n",
    "    edited_model, \n",
    "    tokenizer, \n",
    "    generation_prompts, \n",
    "    max_out_len=100\n",
    ")\n",
    "\n",
    "print(post_update_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (prompt, pre, post) in enumerate(\n",
    "        zip(generation_prompts, pre_update_text, post_update_text)\n",
    "    ):\n",
    "        if i > 0:\n",
    "            print(\"\".join([\"-\" for _ in range(10)]))\n",
    "\n",
    "        prompt_str = \"[Prompt]:\"\n",
    "        pre_str = f\"[Pre]:\"\n",
    "        post_str = f\"[Post]:\"\n",
    "        pad_to = 1 + max(len(prompt_str), len(pre_str), len(post_str))\n",
    "\n",
    "        for s, t in zip([prompt_str, post_str, pre_str], [prompt, post, pre]):\n",
    "            print(s.ljust(pad_to), t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('rome')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f499f353d5b34d56f376cf9ea9e3b3fc7c5bcd20bdb42278266190834e209eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
